---
title: 'Math 189: Multiple Testing III'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/neide/Documents/GitHub/ma189/Data')
```
 
# General Two-Sample Testing Problem

- In general, observations from the two samples may not be one-to-one paired. We may even observe two samples with very different sizes.

## Sample 1

- $n_1$ observations
- $m$ variables
- $\underline{x}_1^{(1)}, \ldots, \underline{x}_{n_1}^{(1)}$.
- Population parameters: $\underline{\mu}^{(1)}$, ${\mathbf \Sigma}^{(1)}$.
- Sample statistics: $\overline{\underline{x}}^{(1)}$, ${\mathbf S}^{(1)}$.

## Sample 2

- $n_2$ observations
- $m$ variables
- $\underline{x}_1^{(2)}, \ldots, \underline{x}_{n_2}^{(2)}$.
- Population parameters: $\underline{\mu}^{(2)}$, ${\mathbf \Sigma}^{(2)}$.
- Sample statistics: $\overline{\underline{x}}^{(2)}$, ${\mathbf S}^{(2)}$.

We are interested in testing the equivalence of the two population means:
\[
 H_0: \underline{\mu}^{(1)} = \underline{\mu}^{(2)} \quad H_a: \underline{\mu}^{(1)} \neq \underline{\mu}^{(2)},
\]

## Two-Sample Hotelling’s $T^2$

- The two-sample Hotelling’s $T^2$  statistic is defined as
\[
 T^2 = {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}^{\prime}
  \, { \left( n_1^{-1} {\mathbf S}^{(1)} + n_2^{-1} {\mathbf S}^{(2)} \right)}^{-1} \,
  {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}.
\]
- This is a function of the difference between the sample means for the two populations.
- Instead of being a function of a single covariance matrix, we can see that the two-sample
test statistic is a function of ${\mathbf S}^{(1)}$,  ${\mathbf S}^{(2)}$, $n_1$, and $n_2$.
- Denote
\[
  {\mathbf S}_T =  n_1^{-1} {\mathbf S}^{(1)} + n_2^{-1} {\mathbf S}^{(2)},
\]
which is a weighted-average of sample covariance matrices  ${\mathbf S}^{(1)}$ and
${\mathbf S}^{(2)}$.

## Two-Sample Hotelling’s $T^2$ (Large Sample)

- When both $n_1$ and $n_2$ are large, we can ignore the effects of estimating the
population covariance matrices by their sample estimators:
\[
 {\mathbf S}^{(1)} \approx {\mathbf \Sigma}^{(1)}, \quad
  {\mathbf S}^{(2)} \approx {\mathbf \Sigma}^{(2)}.
\]
- Therefore the $T^2$ statistic is approximately chi-square distributed with $m$ degrees of
freedom, under the null hypothesis.
\[
 T^2 \approx {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}^{\prime}
  \, { \left( n_1^{-1} {\mathbf \Sigma}^{(1)} + 
  n_2^{-1} {\mathbf \Sigma}^{(2)} \right)}^{-1} \,
  {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}
  \sim \chi^2_{m}.
\]
- At a given significance level $\alpha$, we reject the null hypothesis $H_0$ if
\[
 T^2 > \chi^2_{m,\alpha}.
 \]
 
## Two-Sample Hotelling’s $T^2$ (Small Sample)

- When the sample sizes $n_1$ and $n_2$ are not large, we cannot ignore the effects of
estimating the population covariance matrices by their sample counterparts.
- Instead, we can calculate an $F$ transformation using the formula
\[
 F = \frac{ n_1 + n_2 - m -1}{m (n_1 + n_2 -2)} \, T^2 \sim \mathcal{F}_{m,\nu},
\]
 where the degree of freedom $\nu$  is provided by the following complicated formula:
\[
 \frac{1}{\nu} = \sum_{k=1}^2 \frac{1}{n_k -1}
 { \left( \frac{  {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}^{\prime} 
 {\mathbf S}_T^{-1}
 \left( n_k^{-1} {\mathbf S}^{(k)} \right)  {\mathbf S}_T^{-1} {( \overline{\underline{x}}^{(1)} - \overline{\underline{x}}^{(2)} )}
  }{ T^2 } \right) }^2.
\]
- At a given significance level $\alpha$, we reject the null hypothesis $H_0$ if
\[
F > \mathcal{F}_{m,\nu,\alpha}.
\]

## Two-Sample Hotelling’s $T^2$  (${\mathbf \Sigma}^{(1)} = {\mathbf \Sigma}^{(2)}$)

- The two-sample Hotelling’s $T^2$ statistic can be simplified if we know the two
populations share the same variance-covariance matrix, i.e.
\[
{\mathbf \Sigma}^{(1)} = {\mathbf \Sigma}^{(2)}.
\]
- In this case, we can use the pooled variance estimator
\[
  {\mathbf S}_P = \left( \frac{1}{n_1} + \frac{1}{n_2} \right) \,
    \left( \frac{n_1 -1}{n_1 + n_2 -2} {\mathbf S}^{(1)} + 
   \frac{n_2 -1}{n_1 + n_2 -2} {\mathbf S}^{(2)} \right).
\]
 This gives a slightly different formula from ${\mathbf S}_T$.
- When the sample sizes $n_1$ and $n_2$ are not large, the $F$-statistic will be
\[
 F = \frac{ n_1 + n_2 - m -1}{m (n_1 + n_2 -2)} \, T^2 \sim \mathcal{F}_{m,n_1+n_2 - m-1}.
\]
In other words, the degrees of freedom $\nu$ equals $n_1+n_2 - m-1$ in this case. 
- At a given significance level $\alpha$, we reject the null hypothesis $H_0$ if
\[
F > \mathcal{F}_{m,n_1+n_2 - m-1,\alpha}.
\]

## Two-Sample Hotelling’s $T^2$ Summary

- When variances are different (${\mathbf \Sigma}^{(1)} \neq {\mathbf \Sigma}^{(2)}$), use $S_T$ as the variance estimate.
- When variances are equal (${\mathbf \Sigma}^{(1)} = {\mathbf \Sigma}^{(2)}$), use $S_P$ as the variance estimate.
- When sample sizes are large enough, there is an asymptotic $\chi^2$ distribution.
- For smaller sample sizes, a rescaled $T^2$ has a F distribution.

## Example: Swiss Bank Notes

- This is a dataset that contains six variables measured on 100 genuine and 100 counterfeit old Swiss 1000-franc bank notes.
- Six variables are measured:

1. Length of the note
2. Width of the Left-Hand side of the note
3. Width of the Right-Hand side of the note
4. Width of the Bottom Margin
5. Width of the Top Margin
6. Diagonal Length of Printed Area

![SwissFrance](images/swiss_franc.jpg)

### Objective: 

- Determine if counterfeit notes can be distinguished from the genuine Swiss bank notes.
- One way to answer this question is to test if the genuine and counterfeit notes have different population means.
- Denote the population mean vectors of genuine and counterfeit populations as $\underline{\mu}^{(1)}$ and $\underline{\mu}^{(2)}$ respectively. Denote by $\overline{\underline{x}}^{(1)}$ and $\overline{\underline{x}}^{(2)}$ the sample means, and
${\mathbf S}^{(1)}$ and ${\mathbf S}^{(2)}$ the sample variance matrices.
- We are interested in testing the equivalence of the two population means:
\[
 H_0: \underline{\mu}^{(1)} = \underline{\mu}^{(2)} \quad H_a: \underline{\mu}^{(1)} \neq \underline{\mu}^{(2)}.
\]
- To simplify our calculation, let us first assume the two populations share the
same population variance-covariance matrices.  **This assumption could be wrong!**
\[
{\mathbf \Sigma}^{(1)} = {\mathbf \Sigma}^{(2)}.
\]

### A Peek at the Dataset

First, let us have a peak at the dataset. BN1 to BN100 are genuine banknotes and BN101 to BN200 are counterfeit banknotes. The data is from "Multivariate Statistics: A practical approach", by Bernhard Flury and Hans Riedwyl, Chapman and Hall, 1988.

```{r}
swiss <- read.table("SBN.txt")
swiss[1:5,]
swiss[101:105,]
```

### Testing Procedure

- We calculate the sample means of each variable from the genuine and counterfeit samples, respectively.  

```{r}
mu_mat <- cbind(colMeans(swiss[1:100,]),colMeans(swiss[101:200,]))
colnames(mu_mat) <- c("Genuine Sample Mean","Counterfeit Sample Mean")
mu_mat
```

- We calculate the sample variance covariance matrices from the genuine and counterfeit samples, respectively.

```{r}
var_genuine <- var(swiss[1:100,])
var_counterfeit <- var(swiss[101:200,])
var_genuine
var_counterfeit
```

- Then we calculate
\[
  {\mathbf S}_T =  n_1^{-1} {\mathbf S}^{(1)} + n_2^{-1} {\mathbf S}^{(2)},
\]

```{r}
n1 <- 100
n2 <- 100
var_weighted <- n1^{-1} * var_genuine + n2^{-1} * var_counterfeit
round(var_weighted,digits=6)
```

- Get the $T^2$ and $F$ statistics:

```{r}
z <- swiss[1:100,] - swiss[101:200,]
hotel <- t(colMeans(z)) %*% solve(var_weighted) %*% (colMeans(z))
hotel
m <- 6
alpha <- .05
f_stat <- (n1+n2-m-1)/(m*(n1+n2-2)) * hotel
f_stat
qf(1-alpha,df1=m,df2=n1+n2-m-1)
```

- Decision: reject $H_0$!  Conclusion: the counterfeit notes can be distinguished from the genuine notes on at least one of the measurements.

### Alternative Calculation

- We can use the pooled variance estimator instead:
\[
  {\mathbf S}_P = \left( \frac{1}{n_1} + \frac{1}{n_2} \right) \,
    \left( \frac{n_1 -1}{n_1 + n_2 -2} {\mathbf S}^{(1)} + 
   \frac{n_2 -1}{n_1 + n_2 -2} {\mathbf S}^{(2)} \right).
\]
 (This equals ${\mathbf S}_T$ when $n_1 = n_2$.)
 
```{r}
var_pooled <- (1/n1 + 1/n2)*(n1+n2-2)^{-1}*
  ((n1-1)*var_genuine + (n2-1)*var_counterfeit)
round(var_pooled,digits=6)
hotel <- t(colMeans(z)) %*% solve(var_pooled) %*% (colMeans(z))
hotel
m <- 6
alpha <- .05
f_stat <- (n1+n2-m-1)/(m*(n1+n2-2)) * hotel
f_stat
qf(1-alpha,df1=m,df2=n1+n2-m-1)
```

### Getting Fancy with Packages

```{r}
library(Hotelling)
library(ICSNP)
X1 <- swiss[1:100,]
X2 <- swiss[101:200,]
colMeans(X1)
colMeans(X2)
S1 <- cov(X1)
S2 <- cov(X2)

# Package number one
hot_test <- hotelling.test(X1,X2)
hot_test

# Package number two
hot_test <- HotellingsT2(X1,X2)
hot_test
```


